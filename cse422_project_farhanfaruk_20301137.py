# -*- coding: utf-8 -*-
"""CSE422 Project_FarhanFaruk_20301137

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ponn9Gj381OpDqznmxHYIMMDqPdB4LdN
"""

# All Library or module

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

from google.colab import drive
drive.mount('/content/drive')

# Load Dataset
import os
directory = "/content/drive/MyDrive/CSE 422 Project/"
os.chdir(directory)

!ls

dataset = pd.read_csv('Heart 2020 Dataset.csv')

dataset.shape

# Number of features
print(len(dataset.columns))
a = list(dataset.columns)
print(a)

# Inspect the data to determine the type of class/label
for i in a:
  class_labels = dataset[i]
  unique_labels = class_labels.unique()
  print(f"Unique class/label values of column {i}:", unique_labels)

#Data Points

# Get the number of data points using the shape attribute
num_data_points = dataset.shape[0]

# len() function to get the number of rows or data points
num_data_points = len(dataset)

# Print the number of data points
print("Number of data points:", num_data_points)

# types of features

# Get the unique column names or types of features
unique_features = dataset.columns.tolist()

# Get the number of unique column names or types of features
num_unique_features = len(unique_features)

# Print the unique column names or types of features and the number of unique features
print("Unique column names or types of features:", unique_features)
print("Number of unique features:", num_unique_features)

# Correlation of the features along with the label/class
# Calculate the correlation matrix
correlation_matrix = dataset.corr()

# Create a heatmap of the correlation matrix using seaborn
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')

# Set the title of the plot
plt.title('Correlation of Features with Label/Class')

# Display the plot
plt.show()

# Bias / Balanced

lst = ['HeartDisease', 'Height', 'BMI', 'Smoking', 'AlcoholDrinking', 'Stroke', 'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory', 'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'SleepTime', 'Asthma', 'KidneyDisease', 'SkinCancer']

for i in lst:
  # Check for class balance
  class_counts = dataset[i].value_counts()  # Replace 'target_column' with the column name for the target variable in your dataset
  class_labels = class_counts.index
  class_frequencies = class_counts / class_counts.sum()
  class_balance = class_frequencies.max() / class_frequencies.min()


  plt.bar(class_labels, class_frequencies)
  plt.xlabel(f'{i} Labels')
  plt.ylabel(f'{i} Frequencies')
  plt.title(f'{i} Balance')
  plt.show()

  column = 'HeartDisease'  # Replace with the column name for gender in your dataset
  counts = dataset[column].value_counts()
  labels = counts.index
  frequencies = counts / counts.sum()

  plt.bar(labels, frequencies)
  plt.xlabel(f'{i} Labels')
  plt.ylabel(f'{i} Frequencies')
  plt.title(f'Feature Bias ({i})')
  plt.show()

dataset.isnull()

dataset.isnull().sum()

nan_attr = 0
for e in dataset.isnull().sum():
  if e>0:
    nan_attr += 1
print(nan_attr)

# Dropping Column

list_of_columns_to_drop =["Height"]
dataset = dataset.drop(list_of_columns_to_drop, axis = 1) # 1 means column

# Dropping Rows
print("Shape before removing null values: ", dataset.shape)
dataset = dataset.dropna(axis=0, subset=list(dataset.columns))
print("Shape after removing null values: ", dataset.shape)

dataset.info()

object_type_list = ["HeartDisease","Smoking","AlcoholDrinking","Stroke","DiffWalking","Sex","Race","AgeCategory","Diabetic","PhysicalActivity","GenHealth","Asthma","KidneyDisease","SkinCancer"]
for i in object_type_list:
  enc = LabelEncoder()
  dataset[i] = enc.fit_transform(dataset[i])

dataset.info()

dataset.isnull().sum()

# Download new data set
dataset.to_csv('Dataset v2.csv', index=False)

# Correlation of the features along with the label/class
# Calculate the correlation matrix
correlation_matrix = dataset.corr()

# Create a heatmap of the correlation matrix using seaborn
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')

# Set the title of the plot
plt.title('Correlation of Features with Label/Class')

# Display the plot
plt.show()

#create independent and Dependent Feature
columns = dataset.columns.tolist()
#Filter the columns to remove data we  do not want
columns = [c for c in columns if c not in ["HeartDisease"]]
#store the var we are predicting
target = "HeartDisease"
#define a random state
state = np.random.RandomState(42)
#feature value
X = dataset[columns]
#target value
Y = dataset[target]
x_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))

# Data Balancing
under_sampler = RandomUnderSampler()
x, y = under_sampler.fit_resample(X, Y)
x.shape, y.shape

# Split the dataset between Train and Test Set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)
print (x_train. shape, x_test.shape, y_train. shape, y_test.shape)

# Scalling
scaler = MinMaxScaler()
scaler.fit(x_train)
X_train_scaled = scaler.transform(x_train)
X_test_scaled = scaler.transform(x_test)

# Logistic Regression

Logisctic = LogisticRegression(max_iter=1000)
Logisctic.fit(X_train_scaled, y_train)

y_pred = Logisctic.predict(X_test_scaled)
Logisctic_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", Logisctic_accuracy)
print()
print("======================================================")
print(classification_report(y_test,y_pred))

# confusion Maxtrix
cm1 = confusion_matrix(y_test, y_pred)
sns.heatmap(cm1/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')

# Decision tree

Decision_model = DecisionTreeClassifier()

Decision_model.fit(X_train_scaled, y_train)
y_pred = Decision_model.predict(X_test_scaled)

Decision_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", Decision_accuracy)
print()
print("======================================================")
print(classification_report(y_test,y_pred))

# confusion Maxtrix
cm2 = confusion_matrix(y_test, y_pred)
sns.heatmap(cm2/np.sum(cm2), annot = True, fmt=  '0.2%', cmap = 'Reds')

# Kth Nearest Neighbor

knn = KNN(n_neighbors=5)

knn.fit(X_train_scaled, y_train)
y_pred = knn.predict(X_test_scaled)

knn_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", knn_accuracy)
print()
print("======================================================")
print(classification_report(y_test,y_pred))

# confusion Maxtrix
cm3 = confusion_matrix(y_test, y_pred)
sns.heatmap(cm3/np.sum(cm3), annot = True, fmt=  '0.2%', cmap = 'Reds')

# SVM

svm = SVC(kernel='linear')
svm.fit(X_train_scaled, y_train)
y_pred = svm.predict(X_test_scaled)

svm_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", svm_accuracy)
print()
print("======================================================")
print(classification_report(y_test,y_pred))

# confusion Maxtrix
cm4 = confusion_matrix(y_test, y_pred)
sns.heatmap(cm4/np.sum(cm4), annot = True, fmt=  '0.2%', cmap = 'Reds')

# Naive Bayes Classifier

naive_bayes = GaussianNB()
naive_bayes.fit(X_train_scaled, y_train)
y_pred = naive_bayes.predict(X_test_scaled)

naive_bayes_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", naive_bayes_accuracy)
print()
print("======================================================")
print(classification_report(y_test,y_pred))

# confusion Maxtrix
cm5 = confusion_matrix(y_test, y_pred)
sns.heatmap(cm5/np.sum(cm5), annot = True, fmt=  '0.2%', cmap = 'Reds')

# Model selection/Comparison analysis

error_rate_knn = 1 - knn_accuracy
error_rate_dt = 1 - Decision_accuracy
error_rate_svm = 1 - svm_accuracy
error_rate_lr = 1 - Logisctic_accuracy
error_rate_nb = 1 - naive_bayes_accuracy

models = ['KNN', 'Decision Tree', 'SVM', 'Logistic Regression', 'Naive Bayes']
accuracies = [knn_accuracy, Decision_accuracy, svm_accuracy, Logisctic_accuracy, naive_bayes_accuracy]
error_rates = [error_rate_knn, error_rate_dt, error_rate_svm, error_rate_lr, error_rate_nb]

fig, ax = plt.subplots(figsize=(8, 6))
ax.bar(models, accuracies)
ax.set_xlabel('Model')
ax.set_ylabel('Accuracy')
ax.set_title('Accuracy of Different Models')


for i in range(len(models)):
    ax.text(models[i], accuracies[i] + 0.01, f'{accuracies[i]:.2f}', ha='center')
    ax.text(models[i], 0.95, f'{error_rates[i]:.2f}', ha='center')

plt.show()

# Test on an unseen instance


unseen_instance = np.array([[35.15,	1,	0,	0,	0,	0,	0,	1,	10,	5,	0,	1,	2,	8,	0,	0, 0]	])
pred_knn = knn.predict(unseen_instance)
pred_dt = Decision_model.predict(unseen_instance)
pred_svm = svm.predict(unseen_instance)
pred_lr = Logisctic.predict(unseen_instance)
pred_nb = naive_bayes.predict(unseen_instance)

print("Prediction for an unseen instance:")
print(f"KNN: {pred_knn}")
print(f"Decision Tree: {pred_dt}")
print(f"SVM: {pred_svm}")
print(f"Logistic Regression: {pred_lr}")
print(f"Naive Bayes: {pred_nb}")

# Test on a set of unseen instances
unseen_instances = np.array([[32.36,	0,	0,	0,	0,	0,	0,	1,	10,	5,	2,	1,	0,	8,	1,	0,0], [32.61,	1,	0,	0,	0,	0,	0,	0,	9,	5,	0,	1,	1,	5,	1,	0,	1], [26.04,	0,	0,	0,	0,	0,	0,	0,	11,	2,	2,	1,	4,	7,	0,	0,	0]])
preds_knn = knn.predict(unseen_instances)
preds_dt = Decision_model.predict(unseen_instances)
preds_svm = svm.predict(unseen_instances)
preds_lr = Logisctic.predict(unseen_instances)
preds_nb = naive_bayes.predict(unseen_instances)

print("Predictions for set of unseen instances:")
print(f"KNN: {preds_knn}")
print(f"Decision Tree: {preds_dt}")
print(f"SVM: {preds_svm}")
print(f"Logistic Regression: {preds_lr}")
print(f"Naive Bayes: {preds_nb}")